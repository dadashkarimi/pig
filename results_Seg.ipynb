{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98eadc18-b8f7-4057-9134-2ad7399a6992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cbica/home/dadashkj/neurite/neurite/__init__.py:33: FutureWarning: The default backend will soon be changing to 'pytorch'. If you prefer to use TensorFlow, please set the NEURITE_BACKEND environment variable to 'tensorflow'.\n",
      "  backend = py.utils.get_backend()\n",
      "/cbica/home/dadashkj/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 11:29:01.860172: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-05-21 11:29:01.860267: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: cubic-login5\n",
      "2025-05-21 11:29:01.860288: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: cubic-login5\n",
      "2025-05-21 11:29:01.860466: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.73.1\n",
      "2025-05-21 11:29:01.860567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.73.1\n",
      "2025-05-21 11:29:01.860586: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.73.1\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from neurite_sandbox.tf.models import labels_to_labels\n",
    "from neurite_sandbox.tf.utils.augment import add_outside_shapes\n",
    "from neurite.tf.utils.augment import draw_perlin_full\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import scipy.ndimage as ndi\n",
    "import tensorflow.keras.layers as KL\n",
    "import voxelmorph as vxm\n",
    "\n",
    "\n",
    "import argparse\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import pathlib\n",
    "# import surfa as sf\n",
    "import re\n",
    "import json\n",
    "from keras import backend as K\n",
    "import param_3d\n",
    "import data\n",
    "import model_3d\n",
    "from data_3d import *\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from utils import *\n",
    "from help import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Check devices\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "def get_pig_model(k1,k2):\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_\"+str(k1)+\"_\"+str(k2), 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_hmrf_model(k1,k2):\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_hmrf_\"+str(k1)+\"_\"+str(k2), 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_original(k1,k2):\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_\"+str(k1)+\"_\"+str(k2)+\"_orig\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_128_original():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_6_6_128_orig\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_96_full():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,7]\n",
    "    input_img = Input(shape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_seg_FULL_6_6_96\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_atlas():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,251]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_seg_atlas_6_6\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_atlas_new():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,103]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_seg_atlas_6_6_new\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "    \n",
    "def get_pig_model_96():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,3]\n",
    "    input_img = Input(shape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_seg_6_6_96\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_hmrf_model_128():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_hmrf_6_6_128\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_128():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_6_6_128\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "    \n",
    "def get_pig_model_binary_96():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_96,param_3d.img_size_96,param_3d.img_size_96, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_6_6_96\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "\n",
    "def get_pig_model_binary_128():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,2]\n",
    "    input_img = Input(shape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_128,param_3d.img_size_128,param_3d.img_size_128, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_6_6_128\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n",
    "    \n",
    "def get_pig_model_atlas_new():\n",
    "    epsilon =1e-7\n",
    "    min_max_norm = Lambda(lambda x: (x - K.min(x)) / (K.max(x) - K.min(x)+ epsilon) * (1.0) )\n",
    "    \n",
    "    print(\"model is loading\")\n",
    "    en = [16 ,16 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64 ,64]\n",
    "    de = [64 ,64 ,64 ,64, 64 ,64 ,64, 64, 64, 16 ,16 ,103]\n",
    "    input_img = Input(shape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1))\n",
    "    unet_model = vxm.networks.Unet(inshape=(param_3d.img_size_192,param_3d.img_size_192,param_3d.img_size_192, 1), nb_features=(en, de),\n",
    "                       nb_conv_per_level=2,\n",
    "                       final_activation_function='softmax')\n",
    "        \n",
    "    latest_weight = max(glob.glob(os.path.join(\"models_gmm_seg_atlas_6_6_new\", 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "    print(latest_weight)\n",
    "    generated_img_norm = min_max_norm(input_img)\n",
    "    segmentation = unet_model(generated_img_norm)\n",
    "    combined_model = Model(inputs=input_img, outputs=segmentation)\n",
    "    combined_model.load_weights(latest_weight)\n",
    "    return combined_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe47a3e-b6ba-41f9-94ff-6ed9e25464e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loading\n",
      "models_gmm_6_6/weights_epoch_80.h5\n",
      "model is loading\n",
      "models_hmrf_6_6/weights_epoch_20.h5\n",
      "model is loading\n",
      "models_gmm_6_6_128/weights_epoch_240.h5\n",
      "model is loading\n",
      "models_hmrf_6_6_128/weights_epoch_1340.h5\n",
      "model is loading\n",
      "models_gmm_6_6_96/weights_epoch_140.h5\n",
      "model is loading\n",
      "models_gmm_seg_atlas_6_6_new/weights_epoch_60.h5\n"
     ]
    }
   ],
   "source": [
    "k1=6\n",
    "k2=6\n",
    "validation_folder_path = \"/cubic/projects/Pig_TBI/JohnWolf/Protocols/T1_mask\"\n",
    "\n",
    "subfolders = [f.name for f in os.scandir(validation_folder_path) if f.is_dir()]\n",
    "combined_model = get_pig_model(k1,k2)\n",
    "\n",
    "pig_hmrf_model = get_pig_hmrf_model(k1,k2)\n",
    "# combined_model = get_pig_model_original(k1,k2)\n",
    "\n",
    "\n",
    "combined_model_128 = get_pig_model_128()\n",
    "combined_model_hmrf_128 = get_pig_hmrf_model_128()\n",
    "# combined_model_128 = get_pig_model_128_original()\n",
    "\n",
    "# combined_model_96 = get_pig_model_96()\n",
    "combined_model_96 = get_pig_model_binary_96()\n",
    "# combined_model_96 = get_pig_model_atlas()\n",
    "\n",
    "atlas=True\n",
    "combined_model_seg = get_pig_model_atlas_new()\n",
    "# num_forground_classes=102 # 250\n",
    "dial_param = 5 if atlas else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1891708-977d-4308-9100-08c548657df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loading\n",
      "models_gmm_6_6/weights_epoch_80.h5\n",
      "model is loading\n",
      "models_gmm_6_6_128/weights_epoch_240.h5\n",
      "results/JAW-076_1month/image.nii.gz\n",
      "[DEBUG] NiftiArrayIO.load(): no header extensions found!\n",
      "[DEBUG] NiftiArrayIO.load(): no header extensions found!\n",
      "mask shape (280, 192, 352) image shape (280, 192, 352)\n",
      "[DEBUG] NiftiArrayIO.load(): no header extensions found!\n",
      "(192, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import surfa as sf\n",
    "import neurite as ne\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import jaccard_score\n",
    "from utils import find_bounding_box, find_random_bounding_box, apply_gaussian_smoothing, extract_cube\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import binary_erosion, binary_dilation, label\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import binary_dilation\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "\n",
    "from scipy.ndimage import binary_erosion, binary_dilation, label\n",
    "from scipy.ndimage import binary_erosion, binary_dilation\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define a 3D structuring element (e.g., 3x3x3 cube)\n",
    "structure = np.ones((3, 3, 3), dtype=bool)\n",
    "\n",
    "\n",
    "validation_folder_path = \"/cubic/projects/Pig_TBI/JohnWolf/Protocols/T1_mask\"\n",
    "validation_folder_path = \"results\"\n",
    "\n",
    "# validation_folder_path = \"/gpfs/fs001/cbica/home/broodman/Pig_project\"\n",
    "\n",
    "subfolders = [f.name for f in os.scandir(validation_folder_path) if f.is_dir()]\n",
    "random.shuffle(subfolders)\n",
    "\n",
    "\n",
    "combined_model = get_pig_model(k1,k2)\n",
    "combined_model_128 = get_pig_model_128()\n",
    "\n",
    "\n",
    "def majority_vote_binary(masks):\n",
    "    \"\"\"\n",
    "    Combine a list of binary 3D masks into a single majority-voted binary mask.\n",
    "    \n",
    "    Parameters:\n",
    "        masks (List[np.ndarray]): List of 3D numpy arrays (binary masks of shape (Z, Y, X))\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 3D binary mask where each voxel is 1 if majority of masks had 1\n",
    "    \"\"\"\n",
    "    masks_stack = np.stack(masks, axis=0)  # shape: (N, Z, Y, X)\n",
    "    vote_sum = np.sum(masks_stack, axis=0)\n",
    "    majority_threshold = len(masks) // 2 + 1  # majority means > N/2\n",
    "    return (vote_sum >= majority_threshold).astype(np.uint8)\n",
    "    \n",
    "\n",
    "def fill_holes_per_class(mask, labels=None):\n",
    "    filled_mask = np.zeros_like(mask)\n",
    "    if labels is None:\n",
    "        labels = np.unique(mask)\n",
    "        labels = labels[labels != 0]  # skip background\n",
    "\n",
    "    for label in labels:\n",
    "        class_mask = (mask == label)\n",
    "        filled_class = ndi.binary_fill_holes(class_mask)\n",
    "        filled_mask[filled_class] = label\n",
    "\n",
    "    return filled_mask\n",
    "\n",
    "from scipy.ndimage import binary_dilation, binary_erosion\n",
    "\n",
    "def fill_holes_by_dilate_erode(mask, iterations=2):\n",
    "    filled_mask = np.zeros_like(mask)\n",
    "    labels = np.unique(mask)\n",
    "    labels = labels[labels != 0]  # Skip background\n",
    "\n",
    "    for label in labels:\n",
    "        binary = (mask == label)\n",
    "        # Dilate first to close small holes\n",
    "        dilated = binary_dilation(binary, iterations=iterations)\n",
    "        # Erode to restore the shape\n",
    "        cleaned = binary_erosion(dilated, iterations=iterations)\n",
    "        filled_mask[cleaned] = label\n",
    "\n",
    "    return filled_mask\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "# def kmeans_merge_fg_and_prune(img, fg_mask, n_bg_clusters=5):\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import binary_dilation\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, binary_dilation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "\n",
    "def trim_mask(pred_mask, mask_dtype=np.uint8, proximity=2, remove_top_n=1):\n",
    "    \"\"\"\n",
    "    Keeps the dominant class and nearby-attached small classes (within `proximity` voxels),\n",
    "    but excludes the top-N largest non-dominant labels.\n",
    "\n",
    "    Parameters:\n",
    "        pred_mask (np.ndarray): labeled input mask\n",
    "        mask_dtype (np.dtype): output data type (e.g., np.uint8)\n",
    "        proximity (int): voxel distance to include nearby labels\n",
    "        remove_top_n (int): number of largest non-dominant labels to exclude\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: final mask with original labels preserved\n",
    "    \"\"\"\n",
    "    flat = pred_mask.ravel()\n",
    "    labels, counts = np.unique(flat[flat > 0], return_counts=True)\n",
    "    if len(counts) == 0:\n",
    "        return np.zeros_like(pred_mask, dtype=mask_dtype)\n",
    "\n",
    "    dominant_label = labels[np.argmax(counts)]\n",
    "    dominant_mask = (pred_mask == dominant_label)\n",
    "    expanded = binary_dilation(dominant_mask, iterations=proximity)\n",
    "\n",
    "    # Sizes of all other labels\n",
    "    label_sizes = {label: np.sum(pred_mask == label) for label in labels if label != dominant_label}\n",
    "    sorted_labels = sorted(label_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "    excluded_labels = set([lbl for lbl, _ in sorted_labels[:remove_top_n]])\n",
    "\n",
    "    # Initialize final mask with the dominant region\n",
    "    final_mask = np.zeros_like(pred_mask, dtype=mask_dtype)\n",
    "    final_mask[dominant_mask] = dominant_label\n",
    "\n",
    "    # Include small nearby-attached labels\n",
    "    for label in labels:\n",
    "        if label == dominant_label or label in excluded_labels:\n",
    "            continue\n",
    "        label_mask = (pred_mask == label)\n",
    "        if np.any(label_mask & expanded):\n",
    "            final_mask[label_mask] = label\n",
    "\n",
    "    print(\"Dominant label:\", dominant_label)\n",
    "    print(\"Excluded labels (top {}):\".format(remove_top_n), excluded_labels)\n",
    "    print(\"Labels in final mask:\", np.unique(final_mask))\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def extract_labeled_largest_component(labeled_mask):\n",
    "    \"\"\"\n",
    "    Returns a labeled mask where only the region under the largest connected component\n",
    "    (from the binarized version) is preserved. All other areas are set to 0.\n",
    "\n",
    "    Parameters:\n",
    "        labeled_mask (np.ndarray): input mask with integer class labels\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: same shape as input, only largest binary component retained\n",
    "                    with original labels\n",
    "    \"\"\"\n",
    "    binary_mask = (labeled_mask > 0)\n",
    "\n",
    "    # Step 2: Label connected components\n",
    "    connected, num = label(binary_mask)\n",
    "    if num == 0:\n",
    "        return np.zeros_like(labeled_mask)\n",
    "\n",
    "    # Step 3: Find largest component\n",
    "    sizes = np.bincount(connected.ravel())\n",
    "    sizes[0] = 0  # background\n",
    "    largest_label = np.argmax(sizes)\n",
    "    largest_region = (connected == largest_label)\n",
    "\n",
    "    # Step 4: Mask original labels using that region\n",
    "    result = np.where(largest_region, labeled_mask, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "from scipy.ndimage import binary_erosion, distance_transform_edt\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_erosion, distance_transform_edt\n",
    "\n",
    "def prune_mask_by_distance_from_core(mask1, mask2, distance_thresh=10, erosion_iter=2):\n",
    "    \"\"\"\n",
    "    Removes distant voxels in mask1 that are farther than `distance_thresh`\n",
    "    from the eroded core of mask2.\n",
    "\n",
    "    Parameters:\n",
    "        mask1 (np.ndarray): binary mask to prune\n",
    "        mask2 (np.ndarray): reference mask whose core is trusted\n",
    "        distance_thresh (int): distance threshold in voxels\n",
    "        erosion_iter (int): erosion depth to define core from mask2\n",
    "\n",
    "    Returns:\n",
    "        pruned_mask (np.ndarray): mask1 cleaned based on distance to core of mask2\n",
    "    \"\"\"\n",
    "    # Step 1: Erode mask2 to get core\n",
    "    core = binary_erosion(mask2, iterations=erosion_iter)\n",
    "\n",
    "    # Step 2: Compute distance from core\n",
    "    distance_from_core = distance_transform_edt(~core)\n",
    "\n",
    "    # Step 3: Remove mask1 voxels that are too far from core\n",
    "    pruned_mask = (mask1 > 0) & (distance_from_core <= distance_thresh)\n",
    "    \n",
    "    return pruned_mask.astype(np.uint8)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import center_of_mass, distance_transform_edt\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation, label, sum as ndi_sum\n",
    "\n",
    "from scipy.ndimage import binary_dilation, label, distance_transform_edt, sum as ndi_sum\n",
    "\n",
    "\n",
    "def combine_mask_with_dilated_label(pred_192_2, initial_prediction, target_label=93, dilation_iters=50):\n",
    "    \"\"\"\n",
    "    Combines pred_192_2 and initial_prediction into a binary mask.\n",
    "    Uses binarized pred_192_2 in the largest dilated region around label==target_label,\n",
    "    and initial_prediction elsewhere.\n",
    "\n",
    "    Parameters:\n",
    "        pred_192_2: np.ndarray, multi-class segmentation\n",
    "        initial_prediction: np.ndarray, binary mask\n",
    "        target_label: int, label to use as trust anchor (default: 93)\n",
    "        dilation_iters: int, how many voxels to dilate label region\n",
    "\n",
    "    Returns:\n",
    "        final_binary_mask: np.ndarray, 0 for background, 2 for foreground\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Find label==target_label region\n",
    "    label_region = (pred_192_2 == target_label)\n",
    "    if not np.any(label_region):\n",
    "        print(f\"Warning: Label {target_label} not found. Using initial_prediction everywhere.\")\n",
    "        return (initial_prediction > 0).astype(np.int32) * 2\n",
    "\n",
    "    # Step 2: Dilate label region\n",
    "    dilated_region = binary_dilation(label_region, iterations=dilation_iters)\n",
    "\n",
    "    # Step 3: Extract largest connected component from dilated region\n",
    "    labeled_components, num_components = label(dilated_region)\n",
    "    if num_components == 0:\n",
    "        print(f\"Warning: No connected components after dilation. Using initial_prediction everywhere.\")\n",
    "        return (initial_prediction > 0).astype(np.int32) * 2\n",
    "\n",
    "    component_sizes = ndi_sum(dilated_region, labeled_components, index=np.arange(1, num_components + 1))\n",
    "    largest_component_label = np.argmax(component_sizes) + 1  # labels start from 1\n",
    "    trusted_region = (labeled_components == largest_component_label)\n",
    "\n",
    "    # Step 4: Binarize pred_192_2 → 0 or 2\n",
    "    mask_binary = (pred_192_2 > 0).astype(np.int32) * 2\n",
    "\n",
    "    # Step 5: Binarize initial_prediction → 0 or 2\n",
    "    init_binary = (initial_prediction > 0).astype(np.int32) * 2\n",
    "\n",
    "    # Step 6: Merge based on trusted region\n",
    "    final_mask = np.where(trusted_region, mask_binary, init_binary)\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "from scipy.ndimage import binary_erosion, label, sum as ndi_sum\n",
    "\n",
    "from scipy.ndimage import binary_erosion, gaussian_filter, label\n",
    "from scipy.ndimage import sum as ndi_sum\n",
    "\n",
    "from scipy.ndimage import binary_erosion, gaussian_filter, label\n",
    "from scipy.ndimage import sum as ndi_sum\n",
    "import numpy as np\n",
    "\n",
    "def clean_mask(image, mask, border=5, thresh=0.1, sigma=2.8):\n",
    "    m = (mask > 0)\n",
    "    struct = np.ones((3, 3, 3), dtype=bool)  # for 3D erosion\n",
    "    eroded = binary_erosion(m, structure=struct, iterations=border)\n",
    "    m[~eroded & (image < thresh)] = 0\n",
    "    lbl, n = label(m)\n",
    "    if n == 0: return np.zeros_like(mask)\n",
    "    m = (lbl == 1 + np.argmax(ndi_sum(m, lbl, index=np.arange(1, n + 1))))\n",
    "    return (gaussian_filter(m.astype(float), sigma) > 0.5).astype(mask.dtype)\n",
    "\n",
    "\n",
    "\n",
    "def refine_prediction1(crop_img,image, mask, mask2, model, model_128,model_hmrf_128, model_96, folder, new_image_size=(192, 192, 192), margin=0, cube_size=128):\n",
    "    \"\"\"\n",
    "    Refines the segmentation prediction in two steps:\n",
    "    1. Makes an initial prediction.\n",
    "    2. Crops the image based on the prediction and runs the model again.\n",
    "    \n",
    "    Parameters:\n",
    "    - crop_img (ndarray): The input image for prediction.\n",
    "    - mask (ndarray): The binary mask.\n",
    "    - model: The trained segmentation model.\n",
    "    - new_image_size (tuple): The new voxel size for resizing (default is (192, 192, 192)).\n",
    "    - margin (int): The margin to add around the bounding box (default is 10).\n",
    "    - cube_size (int): The size of the bounding cube (default is 32).\n",
    "    \n",
    "    Returns:\n",
    "    - final_prediction_resized (ndarray): The final refined prediction, resized to match the original input size.\n",
    "    \"\"\"\n",
    "\n",
    "    new_voxsize =1\n",
    "    orig_shape = image.shape\n",
    "    voxsize = image.geom.voxsize\n",
    "    folder_path = os.path.join(\"results\", folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    affine = np.array(image.geom.vox2world)\n",
    "    crop_img = image.resize(new_voxsize, method=\"linear\").reshape([192, 192, 192, 1])\n",
    "    # nib.save(nib.Nifti1Image(image.data, affine), os.path.join(folder_path, 'image.nii.gz'))\n",
    "    \n",
    "    # Step 1: Initial Prediction\n",
    "    # Binarize the mask\n",
    "    mask.data[mask.data != 0] = 1\n",
    "    # nib.save(nib.Nifti1Image(mask.astype(np.int32), image.geom.vox2world), os.path.join(folder_path, 'mask.nii.gz'))\n",
    "\n",
    "    # Compute mask center (using the provided find_bounding_box function)\n",
    "    ms = np.mean(np.column_stack(np.nonzero(mask)), axis=0).astype(int)\n",
    "    print(crop_img.shape)\n",
    "    \n",
    "    # Make an initial prediction\n",
    "    prediction_one_hot = model.predict(crop_img[None, ...], verbose=0)\n",
    "    initial_prediction = np.argmax(prediction_one_hot, axis=-1)[0]\n",
    "    ne.plot.volume3D(crop_img, slice_nos=ms)\n",
    "    print(\"orig shape\",orig_shape)\n",
    "    print(\"Initial Prediction Result:\")\n",
    "\n",
    "    labeled, num_components = ndimage.label(initial_prediction > 0)\n",
    "    largest_mask = labeled == np.argmax(ndimage.sum(initial_prediction > 0, labeled, range(num_components + 1)))\n",
    "    initial_prediction = ndi.binary_fill_holes(largest_mask)\n",
    "    initial_prediction = (initial_prediction > 0).astype(np.int32)\n",
    "    initial_prediction = clean_mask(initial_prediction, initial_prediction,border=2, thresh=0.1, sigma=0.6)\n",
    "\n",
    "    crop_img = crop_img*(initial_prediction>0)\n",
    "    zoom_in_factor = 2\n",
    "    cube_zoomed = zoom(crop_img, zoom=zoom_in_factor, order=1)  # linear interpolation\n",
    "    cube_zoomed = sf.Volume(cube_zoomed).reshape((192,192,192))\n",
    "    # ne.plot.volume3D(cube_zoomed)\n",
    "    # ne.plot.volume3D(cube_zoomed, slice_nos=ms)\n",
    "    prediction_cropped_one_hot = combined_model_seg.predict(cube_zoomed[None, ..., None], verbose=0)\n",
    "    pred_192 = np.argmax(prediction_cropped_one_hot, axis=-1)[0]\n",
    "    zoom_out_factor = 0.5\n",
    "    pred_96_zoomed_out = zoom(pred_192, zoom=zoom_out_factor, order=0)\n",
    "    print(\"################## segmentation\")\n",
    "    ne.plot.volume3D(pred_96_zoomed_out,cmaps=['tab20c'])\n",
    "    pred_192_2 = sf.Volume(pred_96_zoomed_out).reshape((192,192,192)).data\n",
    "    from scipy.ndimage import binary_erosion, distance_transform_edt\n",
    "\n",
    "\n",
    "    # pred_192_3 = combine_mask_with_dilated_label2(pred_192_2.astype(np.int32),initial_prediction , prediction_128, \n",
    "    #                                              target_label=93,near_label=62,\n",
    "    #                                              dilation_iters=5)\n",
    "    pred_192_3 = combine_mask_with_dilated_label(pred_192_2.astype(np.int32),initial_prediction, target_label=93, dilation_iters=5)\n",
    "    # pred_192_3 = majority_vote_binary([pred_192_3,initial_prediction, prediction_128,prediction_hmrf_128])\n",
    "    pred_192_3 = clean_mask(crop_img, pred_192_3)\n",
    "    \n",
    "    # binarized = (pred_192_3 > 0).astype(np.int32) * 2\n",
    "    # binarized = fill_holes_by_dilate_erode(binarized, iterations=5)\n",
    "    # resized = sf.Volume(binarized).resize(voxsize, method=\"nearest\").reshape(orig_shape).data\n",
    "    # # nib.save(nib.Nifti1Image(resized, affine), os.path.join(folder_path, 'third_prediction.nii.gz'))\n",
    "    \n",
    "\n",
    "    pred_192_2[pred_192_3==0]=0 \n",
    "    pred_192_2 = fill_holes_per_class(pred_192_2)\n",
    "    nib.save(nib.Nifti1Image(sf.Volume(pred_192_2.astype(np.int32)).resize(voxsize,method=\"nearest\").reshape(orig_shape).data, affine), os.path.join(folder_path, 'seg_prediction.nii.gz'))\n",
    "\n",
    "    \n",
    "    return sf.Volume(pred_192_2.astype(np.int32)).resize(voxsize,method=\"nearest\").reshape((192,192,192)).data\n",
    "\n",
    "\n",
    "def refine_prediction2(crop_img,image, mask, mask2, prediction_seg, model, model_128,model_hmrf_128, model_96, folder,\n",
    "                       orig_voxsize,\n",
    "                       new_image_size=(192, 192, 192), margin=0, cube_size=128):\n",
    "    \"\"\"\n",
    "    Refines the segmentation prediction in two steps:\n",
    "    1. Makes an initial prediction.\n",
    "    2. Crops the image based on the prediction and runs the model again.\n",
    "    \n",
    "    Parameters:\n",
    "    - crop_img (ndarray): The input image for prediction.\n",
    "    - mask (ndarray): The binary mask.\n",
    "    - model: The trained segmentation model.\n",
    "    - new_image_size (tuple): The new voxel size for resizing (default is (192, 192, 192)).\n",
    "    - margin (int): The margin to add around the bounding box (default is 10).\n",
    "    - cube_size (int): The size of the bounding cube (default is 32).\n",
    "    \n",
    "    Returns:\n",
    "    - final_prediction_resized (ndarray): The final refined prediction, resized to match the original input size.\n",
    "    \"\"\"\n",
    "\n",
    "    orig_shape = image.shape\n",
    "    voxsize = image.geom.voxsize\n",
    "    folder_path = os.path.join(\"results\", folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    affine = np.array(image.geom.vox2world)\n",
    "    # nib.save(nib.Nifti1Image(image.data, affine), os.path.join(folder_path, 'image.nii.gz'))\n",
    "    \n",
    "    # Step 1: Initial Prediction\n",
    "    # Binarize the mask\n",
    "    mask.data[mask.data != 0] = 1\n",
    "    # nib.save(nib.Nifti1Image(mask.astype(np.int32), image.geom.vox2world), os.path.join(folder_path, 'mask.nii.gz'))\n",
    "\n",
    "    # Compute mask center (using the provided find_bounding_box function)\n",
    "    ms = np.mean(np.column_stack(np.nonzero(mask)), axis=0).astype(int)\n",
    "    print(crop_img.shape)\n",
    "    \n",
    "    # Make an initial prediction\n",
    "    prediction_one_hot = model.predict(crop_img[None, ...], verbose=0)\n",
    "    initial_prediction = np.argmax(prediction_one_hot, axis=-1)[0]\n",
    "    ne.plot.volume3D(crop_img, slice_nos=ms)\n",
    "    print(\"orig shape\",orig_shape)\n",
    "    print(\"Initial Prediction Result:\")\n",
    "\n",
    "    labeled, num_components = ndimage.label(initial_prediction > 0)\n",
    "    largest_mask = labeled == np.argmax(ndimage.sum(initial_prediction > 0, labeled, range(num_components + 1)))\n",
    "    initial_prediction = ndi.binary_fill_holes(largest_mask)\n",
    "    initial_prediction = (initial_prediction > 0).astype(np.int32)\n",
    "    initial_prediction = clean_mask(crop_img, initial_prediction,border=2, thresh=0.1, sigma=0.6)\n",
    "    \n",
    "    nib.save(nib.Nifti1Image(sf.Volume(initial_prediction.astype(np.int32))\n",
    "                             .reshape(orig_shape).data, affine), os.path.join(folder_path, 'initial_prediction.nii.gz'))\n",
    "\n",
    "    ne.plot.volume3D(initial_prediction, slice_nos=ms)\n",
    "    print(\"first step: \",my_hard_dice(mask.data, initial_prediction))\n",
    "\n",
    "    # initial_prediction_dial = binary_dilation(initial_prediction, structure=structure, iterations=dial_param)\n",
    "    # crop_img = crop_img*(initial_prediction_dial>0)\n",
    "\n",
    "    # # Step 2: Use find_bounding_box function to get the bounding box\n",
    "    # x1, y1, z1, x2, y2, z2 = find_bounding_box(initial_prediction, cube_size=cube_size)\n",
    "    # cube = extract_cube(crop_img, x1, y1, z1, x2, y2, z2, cube_size=128)\n",
    "    # print(\"############################## here \")\n",
    "    # nib.save(nib.Nifti1Image(sf.Volume(cube.astype(np.float32)).reshape(orig_shape).data, affine), os.path.join(folder_path, 'dilated.nii.gz'))\n",
    "\n",
    "    # ne.plot.volume3D(cube, slice_nos=ms)\n",
    "\n",
    "\n",
    "    # pred_192_1 = np.zeros((192,192,192))\n",
    "    # pred_192_2 = np.zeros((192,192,192))\n",
    "    \n",
    "    # ms = np.mean(np.column_stack(np.nonzero(mask)), axis=0).astype(int)\n",
    "    # # ne.plot.volume3D(cube, slice_nos=ms)\n",
    "\n",
    "    # # Step 3: Re-run the Model with the cropped image\n",
    "    # prediction_cropped_one_hot = model_128.predict(cube[None, ...], verbose=0)\n",
    "    # final_prediction = np.argmax(prediction_cropped_one_hot, axis=-1)[0]\n",
    "    # pred_192_1[x1:x2, y1:y2, z1:z2] = final_prediction\n",
    "    # pred_192_1 = fill_holes_by_dilate_erode(pred_192_1, iterations=2)\n",
    "        # pred_192_1 =  = clean_mask(crop_img, pred_192_1,border=2, thresh=0.1, sigma=1.0)\n",
    "    # pred_192_1 = initial_prediction\n",
    "\n",
    "    new_voxsize =1\n",
    "    pred_192_3 = combine_mask_with_dilated_label(prediction_seg.astype(np.int32),initial_prediction, target_label=93, dilation_iters=5)\n",
    "    binarized = (pred_192_3 > 0).astype(np.int32) * 2\n",
    "    binarized = fill_holes_by_dilate_erode(binarized, iterations=2)\n",
    "    resized = sf.Volume(binarized).reshape(orig_shape).data\n",
    "    nib.save(nib.Nifti1Image(resized, affine), os.path.join(folder_path, 'third_prediction.nii.gz'))\n",
    "\n",
    "    tp_fp_map = np.zeros_like(pred_192_3, dtype=np.uint8)\n",
    "    pred_bin = (pred_192_3 > 0).astype(np.int32)\n",
    "    # Apply rules\n",
    "    mask2 = mask2.resize(voxsize).reshape([192, 192, 192, 1])\n",
    "    mask1 = mask.resize(voxsize).reshape([192, 192, 192, 1])\n",
    "    \n",
    "    tp_fp_map[(mask2 == 1)] = 3\n",
    "    tp_fp_map[(mask == 1)] = 2\n",
    "    # tp_fp_map[(pred_bin == 1) & (mask == 0) & (mask2 == 1)] = 2\n",
    "    # tp_fp_map[(pred_bin == 1) & (mask == 1) & (mask2 == 0)] = 3\n",
    "   \n",
    "    \n",
    "    tp_fp_map[(pred_bin == 1) & (mask == 0) & (mask2 == 0)] = 4\n",
    "    tp_fp_map[(pred_bin == 1) & (mask == 1) & (mask2 == 1)] = 1\n",
    "    resized = sf.Volume(tp_fp_map).reshape(orig_shape).data\n",
    "    nib.save(nib.Nifti1Image(resized, affine), os.path.join(folder_path, 'tp_fp_map.nii.gz'))\n",
    "\n",
    "\n",
    "    # if atlas:\n",
    "    #     voxsize=orig_voxsize\n",
    "    #     print(\"orig vs new vox size\",orig_voxsize)\n",
    "    #     new_voxsize =1\n",
    "    #     crop_img = image.resize(new_voxsize, method=\"linear\").reshape([192, 192, 192, 1])\n",
    "    #     # crop_img = crop_img.resize(1).reshape((192,192,192))\n",
    "    #     crop_img = crop_img*(pred_192_1>0)\n",
    "    #     zoom_in_factor = 1\n",
    "    #     cube_zoomed = zoom(crop_img, zoom=zoom_in_factor, order=1)  # linear interpolation\n",
    "    #     cube_zoomed = sf.Volume(cube_zoomed).reshape((192,192,192))\n",
    "    #     # ne.plot.volume3D(cube_zoomed)\n",
    "    #     # ne.plot.volume3D(cube_zoomed, slice_nos=ms)\n",
    "    #     prediction_cropped_one_hot = combined_model_seg.predict(cube_zoomed[None, ..., None], verbose=0)\n",
    "    #     pred_192 = np.argmax(prediction_cropped_one_hot, axis=-1)[0]\n",
    "    #     zoom_out_factor = 1\n",
    "    #     pred_96_zoomed_out = zoom(pred_192, zoom=zoom_out_factor, order=0)\n",
    "    #     print(\"################## segmentation\")\n",
    "    #     ne.plot.volume3D(pred_96_zoomed_out,cmaps=['tab20c'])\n",
    "    #     pred_192_2 = sf.Volume(pred_96_zoomed_out).reshape((192,192,192)).data\n",
    "    #     from scipy.ndimage import binary_erosion, distance_transform_edt\n",
    "\n",
    "\n",
    "    #     # pred_192_3 = combine_mask_with_dilated_label2(pred_192_2.astype(np.int32),initial_prediction , prediction_128, \n",
    "    #     #                                              target_label=93,near_label=62,\n",
    "    #     #                                              dilation_iters=5)\n",
    "    #     pred_192_3 = combine_mask_with_dilated_label(pred_192_2.astype(np.int32),initial_prediction, target_label=93, dilation_iters=5)\n",
    "    #     # pred_192_3 = majority_vote_binary([pred_192_3,initial_prediction, prediction_128,prediction_hmrf_128])\n",
    "    #     pred_192_3 = clean_mask(crop_img, pred_192_3)\n",
    "        \n",
    "    #     binarized = (pred_192_3 > 0).astype(np.int32) * 2\n",
    "    #     binarized = fill_holes_by_dilate_erode(binarized, iterations=5)\n",
    "    #     resized = sf.Volume(binarized).reshape(orig_shape).data\n",
    "    #     nib.save(nib.Nifti1Image(resized, affine), os.path.join(folder_path, 'third_prediction.nii.gz'))\n",
    "        \n",
    "\n",
    "    #     pred_192_2[pred_192_3==0]=0 \n",
    "    #     pred_192_2 = fill_holes_per_class(pred_192_2)\n",
    "    #     nib.save(nib.Nifti1Image(sf.Volume(pred_192_2.astype(np.int32)).reshape(orig_shape).data, affine), os.path.join(folder_path, 'seg_prediction.nii.gz'))\n",
    "\n",
    "\n",
    "        \n",
    "    return pred_192_3>0\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # # nib.save(nib.Nifti1Image(sf.Volume(pred_192.astype(np.int32)).resize(voxsize,method=\"nearest\").reshape(orig_shape).data, affine), os.path.join(folder_path, 'third_prediction.nii.gz'))\n",
    "    # return pred_192_2\n",
    "\n",
    "# Store Dice coefficients\n",
    "dice_scores = []\n",
    "\n",
    "new_voxsize = [0.5, 0.5, 0.5]\n",
    "# new_voxsize = [0.8, 0.8, 0.8]\n",
    "import numpy as np\n",
    "for folder in subfolders:\n",
    "    folder_path = os.path.join(validation_folder_path, folder)\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    folder_path_2 = os.path.join(\"results\", folder)\n",
    "    \n",
    "    # filename = os.path.join(folder_path, f\"{folder_name}_T1.nii.gz\")\n",
    "    # mask_filename = os.path.join(folder_path, f\"{folder_name}_T1_mask.nii.gz\")\n",
    "\n",
    "    filename = os.path.join(folder_path, f\"image.nii.gz\")\n",
    "    mask_filename = os.path.join(folder_path, f\"mask.nii.gz\")\n",
    "\n",
    "\n",
    "    if \"JAW-076_1month\" not in filename:\n",
    "        continue\n",
    "\n",
    "    # if \"JAW-106_6month\" in filename:\n",
    "    #     print(\"NO MASK FOUND\")\n",
    "    #     continue\n",
    "        \n",
    "    print(filename)\n",
    "    \n",
    "\n",
    "\n",
    "    # Skip if no input file\n",
    "    if not os.path.isfile(filename):\n",
    "        continue\n",
    "        \n",
    "    # Load and process image\n",
    "    filename = os.path.join(folder_path_2, 'image.nii.gz')\n",
    "    image = sf.load_volume(filename)\n",
    "    orig_voxsize = image.geom.voxsize\n",
    "    # crop_img = image.resize(new_voxsize, method=\"linear\").reshape([192, 192, 192, 1])\n",
    "    crop_img = image.reshape([192, 192, 192, 1])\n",
    "    orig_shape = image.shape\n",
    "    # orig_voxsize=crop_img.geom.voxsize\n",
    "\n",
    "    # Binarize the mask\n",
    "    \n",
    "\n",
    "        # Load mask\n",
    "    if not os.path.isfile(mask_filename):\n",
    "        mask = sf.Volume(np.ones((192, 192, 192)))\n",
    "    else:\n",
    "        \n",
    "        mask2 = sf.load_volume(mask_filename).reshape(orig_shape)#resize(new_voxsize, method=\"linear\")\n",
    "        affine = np.array(image.geom.vox2world)\n",
    "        print(\"mask shape\",mask2.shape,\"image shape\",image.shape)\n",
    "        nib.save(nib.Nifti1Image(mask2.astype(np.int32), affine), os.path.join(folder_path_2, 'mask.nii.gz'))\n",
    "        mask2 = mask2.reshape([192, 192, 192, 1])\n",
    "\n",
    "        mask_filename = os.path.join(folder_path_2, f\"mask_drew.nii.gz\")\n",
    "        mask = sf.load_volume(mask_filename).reshape(orig_shape)\n",
    "        mask = mask.reshape([192, 192, 192, 1])\n",
    "        \n",
    "    mask.data[mask.data != 0] = 1\n",
    "    mask2.data[mask2.data != 0] = 1\n",
    "\n",
    "    ms = np.mean(np.column_stack(np.nonzero(mask)), axis=0).astype(int)\n",
    "\n",
    "    # prediction_seg = refine_prediction1(crop_img, image, mask, mask2, combined_model,combined_model_128,\n",
    "    #                             combined_model_hmrf_128,\n",
    "    #                             combined_model_96, folder, new_image_size=(192, 192, 192))\n",
    "\n",
    "    prediction = refine_prediction2(crop_img, image, mask, mask2, prediction_seg, pig_hmrf_model,combined_model_128,\n",
    "                                    combined_model_hmrf_128,\n",
    "                                    combined_model_96, folder,orig_voxsize, new_image_size=(192, 192, 192))\n",
    "\n",
    "    # Compute Dice coefficient\n",
    "    mask_flat = mask.data.flatten()\n",
    "    prediction_flat = prediction.flatten()>0\n",
    "    dice_score = 2 * np.sum(mask_flat * prediction_flat) / (np.sum(mask_flat) + np.sum(prediction_flat))\n",
    "    if np.sum(mask.data)<1000:\n",
    "        continue\n",
    "    dice_scores.append(dice_score)\n",
    "\n",
    "    print(f\"Dice coefficient for {folder_name}: {dice_score:.4f}\")\n",
    "    # break\n",
    "\n",
    "# Overall Dice coefficient and visualization\n",
    "overall_dice = np.mean(dice_scores)\n",
    "print(f\"Overall Dice coefficient: {overall_dice:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88da5238-9338-4c64-9e2f-cccab930fe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_voxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494997fb-1ebe-4ff0-ac29-4b8b63d94e1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs_gmm_seg_6_6_96/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m all_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load all events\u001b[39;00m\n\u001b[1;32m     12\u001b[0m event_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, f)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents.out.tfevents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(event_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m event files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m event_files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs_gmm_seg_6_6_96/train/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "log_dir = \"logs_gmm_seg_6_6_96/train/\"\n",
    "all_steps = []\n",
    "all_values = []\n",
    "\n",
    "# Load all events\n",
    "event_files = [\n",
    "    os.path.join(log_dir, f)\n",
    "    for f in os.listdir(log_dir)\n",
    "    if f.startswith(\"events.out.tfevents\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(event_files)} event files.\")\n",
    "\n",
    "for file in event_files:\n",
    "    try:\n",
    "        ea = EventAccumulator(file)\n",
    "        ea.Reload()\n",
    "        if \"loss\" not in ea.Tags().get(\"tensors\", []):\n",
    "            continue\n",
    "        for e in ea.Tensors(\"loss\"):\n",
    "            val = tf.make_ndarray(e.tensor_proto).item()\n",
    "            if not (tf.math.is_nan(val) or tf.math.is_inf(val)):\n",
    "                all_steps.append(e.step)\n",
    "                all_values.append(val)\n",
    "    except Exception as err:\n",
    "        print(f\"Skipping {file}: {err}\")\n",
    "\n",
    "# Sort and subsample\n",
    "combined = sorted(zip(all_steps, all_values), key=lambda x: x[0])\n",
    "steps, values = zip(*combined) if combined else ([], [])\n",
    "\n",
    "# Subsample every 50 points\n",
    "steps_sub = steps[::50]\n",
    "values_sub = values[::50]\n",
    "\n",
    "# Smooth with moving average\n",
    "window = 10\n",
    "smooth_values = np.convolve(values_sub, np.ones(window)/window, mode='valid')\n",
    "smooth_steps = steps_sub[:len(smooth_values)]\n",
    "\n",
    "# Compute standard deviation over rolling window\n",
    "rolling_std = [np.std(values_sub[i:i+window]) for i in range(len(smooth_values))]\n",
    "upper = smooth_values + rolling_std\n",
    "lower = smooth_values - rolling_std\n",
    "\n",
    "# Filter to only show negative losses\n",
    "filtered_steps = []\n",
    "filtered_values = []\n",
    "filtered_upper = []\n",
    "filtered_lower = []\n",
    "\n",
    "for s, v, u, l in zip(smooth_steps, smooth_values, upper, lower):\n",
    "    if v < 0:\n",
    "        filtered_steps.append(s)\n",
    "        filtered_values.append(v)\n",
    "        filtered_upper.append(u)\n",
    "        filtered_lower.append(l)\n",
    "\n",
    "# Plot only negative losses with variance band\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(filtered_steps, filtered_values, label=\"Smoothed Negative Loss\", color='blue')\n",
    "plt.fill_between(filtered_steps, filtered_lower, filtered_upper, color='blue', alpha=0.2, label='±1 std dev')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Brain-Olfactory Segmentation (Negative Loss Only)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"negative_smoothed_loss.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4ecf385-b96a-4a2d-8fa4-c92f63fa4583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dial_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cf3a70a-d72c-4712-a290-bde5be35d815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 1500x500 with 3 Axes>,\n",
       " [array([<AxesSubplot:title={'center':'axis 0'}>,\n",
       "         <AxesSubplot:title={'center':'axis 1'}>,\n",
       "         <AxesSubplot:title={'center':'axis 2'}>], dtype=object)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne.plot.volume3D(mask, slice_nos=ms,cmaps=['tab20c'])\n",
    "ne.plot.volume3D(mask2, slice_nos=ms,cmaps=['tab20c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e527cd0-51c8-4631-9e89-50e702f29dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110680.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cd77b-a11c-4adc-a2ac-c13a399f14df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
